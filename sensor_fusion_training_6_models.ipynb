{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries and Frameworks"
      ],
      "metadata": {
        "id": "2MwohvtboDQ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "Ihx-fNJvgn7_",
        "outputId": "94a17712-8dc3-4d17-fa4d-76f4311c1cf3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdqIRk3wo2qn"
      },
      "outputs": [],
      "source": [
        "#optional installation for torchsummary\n",
        "!pip install torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uudsafoXoZ26"
      },
      "outputs": [],
      "source": [
        "#standard libraries\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "\n",
        "#augmentation\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import albumentations as A\n",
        "\n",
        "#torch\n",
        "import torch\n",
        "from torch.utils.data import Dataset, SubsetRandomSampler, DataLoader, random_split\n",
        "from torch.cuda.amp import GradScaler\n",
        "#from torchvision.transforms import v2\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import StepLR, MultiStepLR, ReduceLROnPlateau, ExponentialLR, CosineAnnealingLR\n",
        "from torchsummary import summary\n",
        "\n",
        "#import onnx\n",
        "import torch.onnx\n",
        "\n",
        "print(f'GPU on: {torch.cuda.is_available()}')\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(DEVICE)\n",
        "_today=datetime.today().strftime('%Y-%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyjkZnIDgnHt"
      },
      "outputs": [],
      "source": [
        "#clear gpu cuda cache\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_yP6oR9gnHu"
      },
      "outputs": [],
      "source": [
        "#clone repo\n",
        "!git clone https://github.com/t0wgster/thesis_constants.git\n",
        "!cd thesis_constants && git pull\n",
        "\n",
        "#load in important functions\n",
        "from thesis_constants.functions_and_constants import *\n",
        "from thesis_constants.functions_and_constants import _WHDataset_10_classes, _WH_HSI_Dataset, _WH_RGB_HSI_Dataset\n",
        "from thesis_constants.visualisation_and_evaluation import *\n",
        "from thesis_constants.models import *\n",
        "from thesis_constants.post_processing import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BdSa9-ZgnHw"
      },
      "outputs": [],
      "source": [
        "#optional\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZyI569ugnHw",
        "outputId": "1d32c31b-f7bb-4cdc-c7ee-4cb29b209ebd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current date (dd-mm-yyyy format): 11-06-2024\n"
          ]
        }
      ],
      "source": [
        "#make training deterministic, important for comparing different models\n",
        "seed_everything(81)\n",
        "VISUALIZE = True\n",
        "\n",
        "# Get the current date and time\n",
        "current_date = datetime.now()\n",
        "\n",
        "# Format the current date as \"dd-mm-yyyy\"\n",
        "CURRENT_DATE = current_date.strftime(\"%d-%m-%Y\")\n",
        "\n",
        "print(\"Current date (dd-mm-yyyy format):\", CURRENT_DATE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftSUByQhw7JY"
      },
      "source": [
        "# Initiate Dataset and Dataloaders for Training/Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDZ9pa6jgnHx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oprjDIG0gnHy"
      },
      "outputs": [],
      "source": [
        "rgb_dir_train = '/content/drive/MyDrive/Master/HSI/train-test-split-1106/split_resized_dataset_rgb_mask_hsi/train/rgb'\n",
        "hsi_dir_train = '/content/drive/MyDrive/Master/HSI/train-test-split-1106/split_resized_dataset_rgb_mask_hsi/train/hsi'\n",
        "mask_dir_train = '/content/drive/MyDrive/Master/HSI/train-test-split-1106/split_resized_dataset_rgb_mask_hsi/train/masks'\n",
        "\n",
        "rgb_dir_test = '/content/drive/MyDrive/Master/HSI/train-test-split-1106/split_resized_dataset_rgb_mask_hsi/test/rgb'\n",
        "hsi_dir_test = '/content/drive/MyDrive/Master/HSI/train-test-split-1106/split_resized_dataset_rgb_mask_hsi/test/hsi'\n",
        "mask_dir_test = '/content/drive/MyDrive/Master/HSI/train-test-split-1106/split_resized_dataset_rgb_mask_hsi/test/masks'\n",
        "\n",
        "rgb_dir_val = '/content/drive/MyDrive/Master/HSI/train-test-split-1106/split_resized_dataset_rgb_mask_hsi/val/rgb'\n",
        "hsi_dir_val = '/content/drive/MyDrive/Master/HSI/train-test-split-1106/split_resized_dataset_rgb_mask_hsi/val/hsi'\n",
        "mask_dir_val = '/content/drive/MyDrive/Master/HSI/train-test-split-1106/split_resized_dataset_rgb_mask_hsi/val/masks'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr-Rk6GBgnHy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "3efdb066-f0f8-4018-b137-fbd157e2948a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name '_WH_RGB_HSI_Dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-185c75283292>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mNUM_WORKERS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m train_dataset = _WH_RGB_HSI_Dataset(rgb_dir_train, hsi_dir_train, mask_dir_train, \n\u001b[0m\u001b[1;32m      6\u001b[0m                                     transform = sf_transformation)\n\u001b[1;32m      7\u001b[0m test_dataset = _WH_RGB_HSI_Dataset(rgb_dir_test, hsi_dir_test, mask_dir_test, \n",
            "\u001b[0;31mNameError\u001b[0m: name '_WH_RGB_HSI_Dataset' is not defined"
          ]
        }
      ],
      "source": [
        "VAL_BATCH_SIZE=12\n",
        "TRAIN_BATCH_SIZE=12\n",
        "NUM_WORKERS=2\n",
        "\n",
        "train_dataset = _WH_RGB_HSI_Dataset(rgb_dir_train, hsi_dir_train, mask_dir_train,\n",
        "                                    transform = sf_transformation)\n",
        "test_dataset = _WH_RGB_HSI_Dataset(rgb_dir_test, hsi_dir_test, mask_dir_test,\n",
        "                                   transform = sf_transformation)\n",
        "val_dataset = _WH_RGB_HSI_Dataset(rgb_dir_val, hsi_dir_val, mask_dir_val,\n",
        "                                  transform = sf_transformation)\n",
        "\n",
        "train_dataset_final = _WH_RGB_HSI_Dataset(rgb_dir_train, hsi_dir_train, mask_dir_train,\n",
        "                                          transform = sf_no_transformation)\n",
        "test_dataset_final = _WH_RGB_HSI_Dataset(rgb_dir_test, hsi_dir_test, mask_dir_test,\n",
        "                                         transform = sf_no_transformation)\n",
        "val_dataset_final = _WH_RGB_HSI_Dataset(rgb_dir_val, hsi_dir_val, mask_dir_val,\n",
        "                                        transform = sf_no_transformation)\n",
        "\n",
        "generator1 = torch.Generator().manual_seed(42)\n",
        "\n",
        "print(f'Train Dataset Length: {len(train_dataset)}')\n",
        "print(f'Test Dataset Length: {len(test_dataset)}')\n",
        "print(f'Validation Dataset Length: {len(val_dataset)}')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE,\n",
        "                          shuffle=True, num_workers=2)\n",
        "train_loader_final = DataLoader(train_dataset_final, batch_size=TRAIN_BATCH_SIZE,\n",
        "                                shuffle=True, num_workers=2)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE,\n",
        "                        shuffle=True, drop_last=True, num_workers=2)\n",
        "val_loader_final = DataLoader(val_dataset_final, batch_size=VAL_BATCH_SIZE,\n",
        "                              shuffle=True, drop_last=True, num_workers=2)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=2,\n",
        "                         shuffle=False, num_workers=2)\n",
        "test_loader_final = DataLoader(test_dataset_final, batch_size=2,\n",
        "                               shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiPBDLt0l4_8"
      },
      "source": [
        "# Check Augmentation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F73xB3qvggiS"
      },
      "outputs": [],
      "source": [
        "print('Legend:')\n",
        "for i, color in enumerate(COLORS_LONG):\n",
        "      print(f'{TXT_COLORS_LONG[i]} -> {CLASSES_LONG[i]}')\n",
        "print('\\033[0m - - - - -')\n",
        "\n",
        "for i in range(0, 20, 2):\n",
        "    print(i)\n",
        "    image, hsi_image, mask = train_dataset[i]\n",
        "    image2, hsi_image2, mask2 = train_dataset[i+1]\n",
        "\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "    hsi_image = hsi_image.numpy().transpose((1, 2, 0))[:,:,0]\n",
        "\n",
        "    image2 = image2.numpy().transpose((1, 2, 0))\n",
        "    hsi_image2 = hsi_image2.numpy().transpose((1, 2, 0))[:,:,0]\n",
        "\n",
        "    #img_arr = np.asarray(image.permute(1,2,0))\n",
        "    mask_arr = np.asarray(mask)\n",
        "\n",
        "    #img_arr2 = np.asarray(image2.permute(1,2,0))\n",
        "    mask_arr2 = np.asarray(mask2)\n",
        "\n",
        "    fig, axs = plt.subplots(1,6, figsize=(16,16))\n",
        "    axs[0].imshow(image)\n",
        "    axs[1].imshow(hsi_image)\n",
        "    axs[2].imshow(mask_arr, cmap=cmap_long, norm=norm_long)\n",
        "\n",
        "    axs[3].imshow(image2)\n",
        "    axs[4].imshow(hsi_image2)\n",
        "    axs[5].imshow(mask_arr2, cmap=cmap_long, norm=norm_long)\n",
        "\n",
        "    for ax in axs:\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tr87M4bewSj"
      },
      "source": [
        "# Training Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfVcFU4jgnHz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "b58640f0-0d62-4589-ae03-2b31f4965753"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3f1c1fd6cdd3>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mPATIENCE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m WEIGHTS = torch.tensor([1.0 ,1.0, 3.0 ,10.0\n\u001b[0m\u001b[1;32m      7\u001b[0m                         \u001b[0;34m,\u001b[0m\u001b[0;36m25.0\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m10.0\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m10.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                         ,12.0 ,1.0 ,10.0]).to(DEVICE)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "LEARNING_RATE = 0.0003\n",
        "NUM_EPOCHS = 300\n",
        "NUM_EPOCHS_FINAL = 3\n",
        "PATIENCE = 100\n",
        "\n",
        "WEIGHTS = torch.tensor([1.0 ,1.0, 3.0 ,10.0\n",
        "                        ,25.0 ,10.0 ,10.0\n",
        "                        ,12.0 ,1.0 ,10.0]).to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Level"
      ],
      "metadata": {
        "id": "K-DPzTVvpN00"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4HUD6RYTo12"
      },
      "source": [
        "# Training Parameters - Data Level Fusion Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_LGQ756hGZj"
      },
      "outputs": [],
      "source": [
        "sf_model = unet_model_gelu_data_level_fusion(in_channels_hsi=6, out_channels=10).to(DEVICE)\n",
        "\n",
        "\n",
        "ce_loss_fn = nn.CrossEntropyLoss(weight=WEIGHTS)\n",
        "dice_loss_fn=DiceLoss(n_classes=10)\n",
        "\n",
        "optimizer = Adam(sf_model.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "scheduler = ExponentialLR(optimizer, last_epoch=-1, gamma=0.9)\n",
        "source = 'sf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBXR403HgnH0"
      },
      "outputs": [],
      "source": [
        "summary(sf_model, input_size=[(3,384,320), (6,384,320)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA2K1qtwXFCV"
      },
      "source": [
        "# Model Training Data Level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJK5jNCnXEi9"
      },
      "outputs": [],
      "source": [
        "avg_train_loss_list=[]\n",
        "avg_val_loss_list=[]\n",
        "avg_train_iou_list=[]\n",
        "avg_val_iou_list=[]\n",
        "softmax = nn.Softmax(dim=1)\n",
        "data_level_name = 'DataLevel'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4lTicZVjgij"
      },
      "outputs": [],
      "source": [
        "trained_model, loss, avg_train_loss_list, avg_val_loss_list = sf_model_training_multiloss(sf_model, train_loader, val_loader_final, NUM_EPOCHS,\n",
        "                                                                            ce_loss_fn, dice_loss_fn, optimizer, scaler, scheduler,\n",
        "                                                                            avg_train_loss_list, avg_val_loss_list,\n",
        "                                                                            TRAIN_BATCH_SIZE, VAL_BATCH_SIZE,\n",
        "                                                                            activate_scheduler=False, patience=PATIENCE, model_name=data_level_name,\n",
        "                                                                            data_source = source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10XqjGd1gnH1"
      },
      "outputs": [],
      "source": [
        "trained_model, loss, avg_train_loss_list, avg_val_loss_list = sf_model_training_multiloss(trained_model, train_loader_final, val_loader_final, NUM_EPOCHS_FINAL,\n",
        "                                                                            ce_loss_fn, dice_loss_fn, optimizer, scaler, scheduler,\n",
        "                                                                            avg_train_loss_list, avg_val_loss_list,\n",
        "                                                                            TRAIN_BATCH_SIZE, VAL_BATCH_SIZE,\n",
        "                                                                            activate_scheduler=False, patience=0, model_name=data_level_name,\n",
        "                                                                            data_source = source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYsQV9JIgnH2"
      },
      "outputs": [],
      "source": [
        "#plot_range = range(NUM_EPOCHS+NUM_EPOCHS_FINAL)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(9,6))\n",
        "ax.plot(range(len(avg_train_loss_list)), avg_train_loss_list, marker='o', linestyle='-', label='Training Loss', color='blue')\n",
        "\n",
        "# Create a twin Axes sharing the xaxis\n",
        "ax2 = ax.twinx()\n",
        "ax2.plot(range(len(avg_val_loss_list)), avg_val_loss_list, marker='o', linestyle='-', label='Validation Loss', color='orange')\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('Training Loss', color='blue')\n",
        "ax2.set_ylabel('Validation Loss', color='orange')\n",
        "ax.set_title('Training vs Validation Loss')\n",
        "\n",
        "# Show legend for both axes\n",
        "ax.legend(loc='upper left')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation - Data Level Fusion Model - Final Model"
      ],
      "metadata": {
        "id": "Yp416no1oq2U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p4WhKXuSgnH2"
      },
      "outputs": [],
      "source": [
        "test_ds_union, test_ds_intersection, test_ds_numerator, test_ds_denominator, iou_image_pixelwise, dice_image_pixelwise = capture_model_metrics_pixelwise_and_confusion_matrix_sf(trained_model, test_dataset_final, data_source='sf', visualize=VISUALIZE, mask_shape = (384, 320))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BlXtXlXTgnH2"
      },
      "outputs": [],
      "source": [
        "calculate_model_metrics(test_ds_intersection, test_ds_union, test_ds_numerator, test_ds_denominator, defects_only=True, file_name=data_level_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n16lmxMKgnH2"
      },
      "source": [
        "# Evaluation - Data Level Fusion Model - Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jwQTbwQMgnH2"
      },
      "outputs": [],
      "source": [
        "\n",
        "path = '/kaggle/working/'\n",
        "model_name = f'best_model_{_today}_{data_level_name}.pt'\n",
        "model_path = os.path.join(path, model_name)\n",
        "best_model_dl = load_model(unet_model_gelu_data_level_fusion(in_channels_hsi=6, out_channels=10), optimizer, scaler, model_path)\n",
        "\n",
        "test_ds_union, test_ds_intersection, test_ds_numerator, test_ds_denominator, iou_image_pixelwise, dice_image_pixelwise = capture_model_metrics_pixelwise_and_confusion_matrix_sf(best_model_dl, test_dataset_final, data_source='sf', visualize=VISUALIZE, mask_shape = (384, 320))\n",
        "\n",
        "calculate_model_metrics(test_ds_intersection, test_ds_union, test_ds_numerator, test_ds_denominator, defects_only=True, file_name=data_level_name)\n",
        "\n",
        "calculate_model_inference_time(best_model_dl, val_loader_final, 'sf')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Level"
      ],
      "metadata": {
        "id": "tOni7OaapRB-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON-szZKSgnH2"
      },
      "source": [
        "# Training Parameters - Feature Level Fusion Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nxjztZqagnH2"
      },
      "outputs": [],
      "source": [
        "sf_model = unet_model_gelu_feature_level_fusion(in_channels_hsi=6, out_channels=10).to(DEVICE)\n",
        "\n",
        "ce_loss_fn = nn.CrossEntropyLoss(weight=WEIGHTS)\n",
        "dice_loss_fn=DiceLoss(n_classes=10)\n",
        "\n",
        "optimizer = Adam(sf_model.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "scheduler = ExponentialLR(optimizer, last_epoch=-1, gamma=0.9)\n",
        "source = 'sf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mQsr_yaognH3"
      },
      "outputs": [],
      "source": [
        "summary(sf_model, input_size=[(3,384,320), (6,384,320)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training Feature Level"
      ],
      "metadata": {
        "id": "rRrXq8IjpJnk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sUJ9YA5vgnH3"
      },
      "outputs": [],
      "source": [
        "avg_train_loss_list=[]\n",
        "avg_val_loss_list=[]\n",
        "avg_train_iou_list=[]\n",
        "avg_val_iou_list=[]\n",
        "softmax = nn.Softmax(dim=1)\n",
        "feature_level_name = 'FeatureLevel'\n",
        "\n",
        "trained_model, loss, avg_train_loss_list, avg_val_loss_list = sf_model_training_multiloss(sf_model, train_loader, val_loader_final, NUM_EPOCHS,\n",
        "                                                                            ce_loss_fn, dice_loss_fn, optimizer, scaler, scheduler,\n",
        "                                                                            avg_train_loss_list, avg_val_loss_list,\n",
        "                                                                            TRAIN_BATCH_SIZE, VAL_BATCH_SIZE,\n",
        "                                                                            activate_scheduler=False, patience=PATIENCE, model_name=feature_level_name,\n",
        "                                                                            data_source = source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b4AnqYOvgnH3"
      },
      "outputs": [],
      "source": [
        "trained_model, loss, avg_train_loss_list, avg_val_loss_list = sf_model_training_multiloss(trained_model, train_loader_final, val_loader_final, NUM_EPOCHS_FINAL,\n",
        "                                                                            ce_loss_fn, dice_loss_fn, optimizer, scaler, scheduler,\n",
        "                                                                            avg_train_loss_list, avg_val_loss_list,\n",
        "                                                                            TRAIN_BATCH_SIZE, VAL_BATCH_SIZE,\n",
        "                                                                            activate_scheduler=False, patience=0, model_name=feature_level_name,\n",
        "                                                                            data_source = source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h9E0HgxLgnH3"
      },
      "outputs": [],
      "source": [
        "#plot_range = range(NUM_EPOCHS+NUM_EPOCHS_FINAL)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(9,6))\n",
        "ax.plot(range(len(avg_train_loss_list)), avg_train_loss_list, marker='o', linestyle='-', label='Training Loss', color='blue')\n",
        "\n",
        "# Create a twin Axes sharing the xaxis\n",
        "ax2 = ax.twinx()\n",
        "ax2.plot(range(len(avg_val_loss_list)), avg_val_loss_list, marker='o', linestyle='-', label='Validation Loss', color='orange')\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('Training Loss', color='blue')\n",
        "ax2.set_ylabel('Validation Loss', color='orange')\n",
        "ax.set_title('Training vs Validation Loss')\n",
        "\n",
        "# Show legend for both axes\n",
        "ax.legend(loc='upper left')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation - Feature Level Fusion Model - Final Model"
      ],
      "metadata": {
        "id": "fiX3YP2dpXsp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "azdYCyY7gnH4"
      },
      "outputs": [],
      "source": [
        "test_ds_union, test_ds_intersection, test_ds_numerator, test_ds_denominator, iou_image_pixelwise, dice_image_pixelwise = capture_model_metrics_pixelwise_and_confusion_matrix_sf(trained_model, test_dataset_final, data_source='sf', visualize=VISUALIZE, mask_shape = (384, 320))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3E6LK03WgnH4"
      },
      "outputs": [],
      "source": [
        "calculate_model_metrics(test_ds_intersection, test_ds_union, test_ds_numerator, test_ds_denominator, defects_only=True, file_name=feature_level_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMkW0NfwgnH4"
      },
      "source": [
        "# Evaluation - Feature Level Fusion Model - Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2TYZnDGignH4"
      },
      "outputs": [],
      "source": [
        "\n",
        "path = '/kaggle/working/'\n",
        "model_name = f'best_model_{_today}_{feature_level_name}.pt'\n",
        "model_path = os.path.join(path, model_name)\n",
        "best_model_fl = load_model(unet_model_gelu_feature_level_fusion(in_channels_hsi=6, out_channels=10), optimizer, scaler, model_path)\n",
        "\n",
        "test_ds_union, test_ds_intersection, test_ds_numerator, test_ds_denominator, iou_image_pixelwise, dice_image_pixelwise = capture_model_metrics_pixelwise_and_confusion_matrix_sf(best_model_fl, test_dataset_final, data_source='sf', visualize=VISUALIZE, mask_shape = (384, 320))\n",
        "\n",
        "calculate_model_metrics(test_ds_intersection, test_ds_union, test_ds_numerator, test_ds_denominator, defects_only=True, file_name=data_level_name)\n",
        "\n",
        "calculate_model_inference_time(best_model_fl, val_loader_final, 'sf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lofxw0sugnH4"
      },
      "source": [
        "# RGB - Unet Classic/Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Parameters - RGB Unet Classic Model"
      ],
      "metadata": {
        "id": "qzvcCa0aph4v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_EFPOal1gnH4"
      },
      "outputs": [],
      "source": [
        "classic_model = unet_model_classic(out_channels=10).to(DEVICE)\n",
        "\n",
        "ce_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = Adam(classic_model.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "scheduler = ExponentialLR(optimizer, last_epoch=-1, gamma=0.9)\n",
        "source = 'rgb'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PSgRix-6gnH5"
      },
      "outputs": [],
      "source": [
        "summary(classic_model, (3, 384, 320))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training RGB Unet Classic"
      ],
      "metadata": {
        "id": "qG_Ih8PNpprj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2hBzhY-fgnH5"
      },
      "outputs": [],
      "source": [
        "avg_train_loss_list=[]\n",
        "avg_val_loss_list=[]\n",
        "avg_train_iou_list=[]\n",
        "avg_val_iou_list=[]\n",
        "softmax = nn.Softmax(dim=1)\n",
        "classic_name = 'Classic'\n",
        "\n",
        "trained_model, loss, avg_train_loss_list, avg_val_loss_list = sf_model_training(classic_model, train_loader, val_loader_final, NUM_EPOCHS,\n",
        "                                                                            ce_loss_fn, optimizer, scaler, scheduler,\n",
        "                                                                            avg_train_loss_list, avg_val_loss_list,\n",
        "                                                                            TRAIN_BATCH_SIZE, VAL_BATCH_SIZE,\n",
        "                                                                            activate_scheduler=False, patience=PATIENCE, model_name=classic_name,\n",
        "                                                                            data_source = source)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6MfTPg8vgnH5"
      },
      "outputs": [],
      "source": [
        "trained_model, loss, avg_train_loss_list, avg_val_loss_list = sf_model_training(trained_model, train_loader_final, val_loader_final, NUM_EPOCHS_FINAL,\n",
        "                                                                            ce_loss_fn, optimizer, scaler, scheduler,\n",
        "                                                                            avg_train_loss_list, avg_val_loss_list,\n",
        "                                                                            TRAIN_BATCH_SIZE, VAL_BATCH_SIZE,\n",
        "                                                                            activate_scheduler=False, patience=0, model_name=classic_name,\n",
        "                                                                            data_source = source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NvdBLcR4gnH5"
      },
      "outputs": [],
      "source": [
        "#plot final training and validation loss\n",
        "fig, ax = plt.subplots(figsize=(9,6))\n",
        "ax.plot(range(len(avg_train_loss_list)), avg_train_loss_list, marker='o', linestyle='-', label='Training Loss', color='blue')\n",
        "\n",
        "# Create a twin Axes sharing the xaxis\n",
        "ax2 = ax.twinx()\n",
        "ax2.plot(range(len(avg_val_loss_list)), avg_val_loss_list, marker='o', linestyle='-', label='Validation Loss', color='orange')\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('Training Loss', color='blue')\n",
        "ax2.set_ylabel('Validation Loss', color='orange')\n",
        "ax.set_title('Training vs Validation Loss')\n",
        "\n",
        "# Show legend for both axes\n",
        "ax.legend(loc='upper left')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation - RGB Unet Classic Model - Final Model"
      ],
      "metadata": {
        "id": "wPwBxRqmps2V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QLCM7fJcgnH6"
      },
      "outputs": [],
      "source": [
        "# evaluate model and visualize model outcome, optionally project confusion matrix\n",
        "test_ds_union, test_ds_intersection, test_ds_numerator, test_ds_denominator, iou_image_pixelwise, dice_image_pixelwise = capture_model_metrics_pixelwise_and_confusion_matrix_sf(trained_model, test_dataset_final, data_source='rgb', visualize=VISUALIZE, mask_shape = (384, 320))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lFW9bVA7gnH6"
      },
      "outputs": [],
      "source": [
        "# calculate average model metrics\n",
        "calculate_model_metrics(test_ds_intersection, test_ds_union, test_ds_numerator, test_ds_denominator, defects_only=True, file_name=classic_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYETtnfJgnH6"
      },
      "source": [
        "# Evaluation - RGB Unet Classic Model - Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7C01gtf6gnH6"
      },
      "outputs": [],
      "source": [
        "# save model and evaluate best model\n",
        "path = '/kaggle/working/'\n",
        "model_name = f'best_model_{_today}_{classic_name}.pt'\n",
        "model_path = os.path.join(path, model_name)\n",
        "best_model_unet = load_model(unet_model_classic(out_channels=10), optimizer, scaler, model_path)\n",
        "\n",
        "test_ds_union, test_ds_intersection, test_ds_numerator, test_ds_denominator, iou_image_pixelwise, dice_image_pixelwise = capture_model_metrics_pixelwise_and_confusion_matrix_sf(best_model_unet, test_dataset_final, data_source='rgb', visualize=VISUALIZE, mask_shape = (384, 320))\n",
        "\n",
        "calculate_model_metrics(test_ds_intersection, test_ds_union, test_ds_numerator, test_ds_denominator, defects_only=True, file_name=classic_name)\n",
        "\n",
        "calculate_model_inference_time(best_model_unet, val_loader_final, 'rgb')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5ujHanngnH6"
      },
      "source": [
        "# RGB - GELU"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Parameters - RGB GELU Model"
      ],
      "metadata": {
        "id": "NHzmgsHCp3xy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vZn2IE7_gnH6"
      },
      "outputs": [],
      "source": [
        "# initiate GEU model with 10 output classes and project model to CUDA\n",
        "gelu_model = unet_model_gelu(out_channels=10).to(DEVICE)\n",
        "\n",
        "# define CE and Dice loss\n",
        "ce_loss_fn = nn.CrossEntropyLoss(weight=WEIGHTS)\n",
        "dice_loss_fn=DiceLoss(n_classes=10)\n",
        "\n",
        "# define Adam optimizer and learning rate\n",
        "optimizer = Adam(gelu_model.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# optional scheduler\n",
        "scheduler = ExponentialLR(optimizer, last_epoch=-1, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4SRGnq41gnH7"
      },
      "outputs": [],
      "source": [
        "#show a model summary\n",
        "summary(gelu_model, (3, 384, 320))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training RGB GELU Model"
      ],
      "metadata": {
        "id": "UOClnpgyp6hV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "bNoaWPDJgnH7",
        "outputId": "c1708a1f-56ea-4b5a-d69e-c51220b7e072"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-feb50a7c9b8c>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mavg_train_iou_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mavg_val_iou_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msoftmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mgelu_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Gelu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "#lists to store train and validation loss, for plotting purposes\n",
        "avg_train_loss_list=[]\n",
        "avg_val_loss_list=[]\n",
        "avg_train_iou_list=[]\n",
        "avg_val_iou_list=[]\n",
        "\n",
        "gelu_name = 'Gelu'\n",
        "\n",
        "# Training for defined number of epochs\n",
        "model, loss, avg_train_loss_list, avg_val_loss_list = sf_model_training_multiloss(gelu_model, train_loader, val_loader_final, NUM_EPOCHS,\n",
        "                                                                            ce_loss_fn, dice_loss_fn, optimizer, scaler, scheduler,\n",
        "                                                                            avg_train_loss_list, avg_val_loss_list,\n",
        "                                                                            TRAIN_BATCH_SIZE, VAL_BATCH_SIZE,\n",
        "                                                                            activate_scheduler=False, patience=PATIENCE, model_name=gelu_name,\n",
        "                                                                            data_source = source)\n",
        "\n",
        "# Training for additional 3 epochs with no augmentations\n",
        "model, loss, avg_train_loss_list, avg_val_loss_list = sf_model_training_multiloss(model, train_loader_final, val_loader_final, NUM_EPOCHS_FINAL,\n",
        "                                                                            ce_loss_fn, dice_loss_fn, optimizer, scaler, scheduler,\n",
        "                                                                            avg_train_loss_list, avg_val_loss_list,\n",
        "                                                                            TRAIN_BATCH_SIZE, VAL_BATCH_SIZE,\n",
        "                                                                            activate_scheduler=False, patience=0, model_name=gelu_name,\n",
        "                                                                            data_source = source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "O65o6vnWgnH7",
        "outputId": "563992f2-642d-4eb0-d4c8-aa3eab5ab1ac"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sf_model_training_multiloss' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-09ce0e942ba9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training for additional 3 epochs with no augmentations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m trained_model, loss, avg_train_loss_list, avg_val_loss_list = sf_model_training_multiloss(trained_model, train_loader_final, val_loader_final, NUM_EPOCHS_FINAL,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                                                             \u001b[0mce_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdice_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                                             \u001b[0mavg_train_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_val_loss_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                             \u001b[0mTRAIN_BATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVAL_BATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sf_model_training_multiloss' is not defined"
          ]
        }
      ],
      "source": [
        "# Training for additional 3 epochs with no augmentations\n",
        "trained_model, loss, avg_train_loss_list, avg_val_loss_list = sf_model_training_multiloss(trained_model, train_loader_final, val_loader_final, NUM_EPOCHS_FINAL,\n",
        "                                                                            ce_loss_fn, dice_loss_fn, optimizer, scaler, scheduler,\n",
        "                                                                            avg_train_loss_list, avg_val_loss_list,\n",
        "                                                                            TRAIN_BATCH_SIZE, VAL_BATCH_SIZE,\n",
        "                                                                            activate_scheduler=False, patience=0, model_name=gelu_name,\n",
        "                                                                            data_source = source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RLRZC2HzgnH7"
      },
      "outputs": [],
      "source": [
        "#plot_range = range(NUM_EPOCHS+NUM_EPOCHS_FINAL)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(9,6))\n",
        "ax.plot(range(len(avg_train_loss_list)), avg_train_loss_list, marker='o', linestyle='-', label='Training Loss', color='blue')\n",
        "\n",
        "# Create a twin Axes sharing the xaxis\n",
        "ax2 = ax.twinx()\n",
        "ax2.plot(range(len(avg_val_loss_list)), avg_val_loss_list, marker='o', linestyle='-', label='Validation Loss', color='orange')\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('Training Loss', color='blue')\n",
        "ax2.set_ylabel('Validation Loss', color='orange')\n",
        "ax.set_title('Training vs Validation Loss')\n",
        "\n",
        "# Show legend for both axes\n",
        "ax.legend(loc='upper left')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation - RGB GELU Model - Final Model"
      ],
      "metadata": {
        "id": "hZ1wOd6zp-KL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bkDOs3AugnH8"
      },
      "outputs": [],
      "source": [
        "test_ds_union, test_ds_intersection, test_ds_numerator, test_ds_denominator, iou_image_pixelwise, dice_image_pixelwise = capture_model_metrics_pixelwise_and_confusion_matrix_sf(trained_model,\n",
        "                                                                                                                                                                                 test_dataset_final,\n",
        "                                                                                                                                                                                 data_source='rgb',\n",
        "                                                                                                                                                                                 visualize = VISUALIZE,\n",
        "                                                                                                                                                                                 mask_shape = (384, 320))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YZkQkndSgnH9"
      },
      "outputs": [],
      "source": [
        "calculate_model_metrics(test_ds_intersection, test_ds_union, test_ds_numerator, test_ds_denominator, defects_only=True, file_name=gelu_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu94LPCbgnH9"
      },
      "source": [
        "# Evaluation - RGB GELU Model - Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NfYuZGfEgnH9"
      },
      "outputs": [],
      "source": [
        "\n",
        "path = '/kaggle/working/'\n",
        "model_name = f'best_model_{_today}_{gelu_name}.pt'\n",
        "model_path = os.path.join(path, model_name)\n",
        "best_model_gelu = load_model(unet_model_gelu(out_channels=10), optimizer, scaler, model_path)\n",
        "\n",
        "test_ds_union, test_ds_intersection, test_ds_numerator, test_ds_denominator, iou_image_pixelwise, dice_image_pixelwise = capture_model_metrics_pixelwise_and_confusion_matrix_sf(best_model_gelu, test_dataset_final, data_source='rgb', visualize=VISUALIZE, mask_shape = (384, 320))\n",
        "\n",
        "calculate_model_metrics(test_ds_intersection, test_ds_union, test_ds_numerator, test_ds_denominator, defects_only=True, file_name=gelu_name)\n",
        "\n",
        "\n",
        "calculate_model_inference_time(best_model_gelu, val_loader_final, 'rgb')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPKT-EBAgnH9"
      },
      "source": [
        "# RGB - ResNet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Parameters - RGB ResNet Model"
      ],
      "metadata": {
        "id": "fiNkkuw2qKXM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ys7kxuLPgnH9",
        "outputId": "e87f9fe7-8e79-4dc8-cdaf-73fae20ebf98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "resnet_model = UNetWithResnet50Encoder(n_classes=10).to(DEVICE)\n",
        "\n",
        "ce_loss_fn = nn.CrossEntropyLoss(weight=WEIGHTS)\n",
        "dice_loss_fn=DiceLoss(n_classes=10)\n",
        "\n",
        "optimizer = Adam(resnet_model.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "scheduler = ExponentialLR(optimizer, last_epoch=-1, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E0k4RzAXgnH9"
      },
      "outputs": [],
      "source": [
        "summary(resnet_model, (3, 384, 320))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training RGB ResNet Model"
      ],
      "metadata": {
        "id": "_7TozebrqOli"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KsfliIBagnH9"
      },
      "outputs": [],
      "source": [
        "avg_train_loss_list=[]\n",
        "avg_val_loss_list=[]\n",
        "avg_train_iou_list=[]\n",
        "avg_val_iou_list=[]\n",
        "softmax = nn.Softmax(dim=1)\n",
        "resnet_name = 'ResNet'\n",
        "\n",
        "trained_model, loss, avg_train_loss_list, avg_val_loss_list = sf_model_training_multiloss(resnet_model, train_loader, val_loader_final, NUM_EPOCHS,\n",
        "                                                                            ce_loss_fn, dice_loss_fn, optimizer, scaler, scheduler,\n",
        "                                                                            avg_train_loss_list, avg_val_loss_list,\n",
        "                                                                            TRAIN_BATCH_SIZE, VAL_BATCH_SIZE,\n",
        "                                                                            activate_scheduler=False, patience=PATIENCE, model_name=resnet_name,\n",
        "                                                                            data_source = source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vcxBUGZ4gnH9"
      },
      "outputs": [],
      "source": [
        "trained_model, loss, avg_train_loss_list, avg_val_loss_list = sf_model_training_multiloss(trained_model, train_loader_final, val_loader_final, NUM_EPOCHS_FINAL,\n",
        "                                                                            ce_loss_fn, dice_loss_fn, optimizer, scaler, scheduler,\n",
        "                                                                            avg_train_loss_list, avg_val_loss_list,\n",
        "                                                                            TRAIN_BATCH_SIZE, VAL_BATCH_SIZE,\n",
        "                                                                            activate_scheduler=False, patience=0, model_name=resnet_name,\n",
        "                                                                            data_source = source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ktmGh0PvgnH9"
      },
      "outputs": [],
      "source": [
        "#plot_range = range(NUM_EPOCHS+NUM_EPOCHS_FINAL)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(9,6))\n",
        "ax.plot(range(len(avg_train_loss_list)), avg_train_loss_list, marker='o', linestyle='-', label='Training Loss', color='blue')\n",
        "\n",
        "# Create a twin Axes sharing the xaxis\n",
        "ax2 = ax.twinx()\n",
        "ax2.plot(range(len(avg_val_loss_list)), avg_val_loss_list, marker='o', linestyle='-', label='Validation Loss', color='orange')\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('Training Loss', color='blue')\n",
        "ax2.set_ylabel('Validation Loss', color='orange')\n",
        "ax.set_title('Training vs Validation Loss')\n",
        "\n",
        "# Show legend for both axes\n",
        "ax.legend(loc='upper left')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation - RGB ResNet Model - Final Model"
      ],
      "metadata": {
        "id": "vnZ-IdJhqRVp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HZxKdKW2gnH9"
      },
      "outputs": [],
      "source": [
        "test_ds_union, test_ds_intersection, test_ds_numerator, test_ds_denominator, iou_image_pixelwise, dice_image_pixelwise = capture_model_metrics_pixelwise_and_confusion_matrix_sf(trained_model, test_dataset_final, data_source='rgb', visualize = VISUALIZE, mask_shape = (384, 320))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NdRnc6TIgnH9"
      },
      "outputs": [],
      "source": [
        "calculate_model_metrics(test_ds_intersection, test_ds_union, test_ds_numerator, test_ds_denominator, defects_only=True, file_name=resnet_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqPUItNcgnH-"
      },
      "source": [
        "# Evaluation - RGB ResNet Model - Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HqOnz2rGgnH_"
      },
      "outputs": [],
      "source": [
        "\n",
        "path = '/kaggle/working/'\n",
        "model_name = f'best_model_{_today}_{resnet_name}.pt'\n",
        "model_path = os.path.join(path, model_name)\n",
        "best_model_resnet = load_model(UNetWithResnet50Encoder(n_classes=10), optimizer, scaler, model_path)\n",
        "\n",
        "test_ds_union, test_ds_intersection, test_ds_numerator, test_ds_denominator, iou_image_pixelwise, dice_image_pixelwise = capture_model_metrics_pixelwise_and_confusion_matrix_sf(best_model_resnet, test_dataset_final, data_source='rgb', visualize=VISUALIZE, mask_shape = (384, 320))\n",
        "\n",
        "calculate_model_metrics(test_ds_intersection, test_ds_union, test_ds_numerator, test_ds_denominator, defects_only=True, file_name=resnet_name)\n",
        "\n",
        "\n",
        "calculate_model_inference_time(best_model_resnet, val_loader_final, 'rgb')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHJLJw1ignH_"
      },
      "source": [
        "# HSI - GELU PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Parameters - HSI GELU Model"
      ],
      "metadata": {
        "id": "ZaB8y11PqY_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wz-2YajjgnH_"
      },
      "outputs": [],
      "source": [
        "hsi_unet_pca_model = hsi_unet_model_gelu_pca(6).to(DEVICE)\n",
        "\n",
        "ce_loss_fn = nn.CrossEntropyLoss(weight=WEIGHTS)\n",
        "dice_loss_fn=DiceLoss(n_classes=10)\n",
        "\n",
        "optimizer = Adam(hsi_unet_pca_model.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "scheduler = ExponentialLR(optimizer, last_epoch=-1, gamma=0.9)\n",
        "source='hsi'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3xq1Qf5NgnH_"
      },
      "outputs": [],
      "source": [
        "summary(hsi_unet_pca_model, (6, 384, 320))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training HSI GELU Model"
      ],
      "metadata": {
        "id": "NSX6suTLqhm7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wVn0KYqcgnH_"
      },
      "outputs": [],
      "source": [
        "avg_train_loss_list=[]\n",
        "avg_val_loss_list=[]\n",
        "avg_train_iou_list=[]\n",
        "avg_val_iou_list=[]\n",
        "softmax = nn.Softmax(dim=1)\n",
        "hsi_unet_pca_name = 'HSI_PCA'\n",
        "\n",
        "trained_model, loss, avg_train_loss_list, avg_val_loss_list = sf_model_training_multiloss(hsi_unet_pca_model, train_loader, val_loader_final, NUM_EPOCHS,\n",
        "                                                                            ce_loss_fn, dice_loss_fn, optimizer, scaler, scheduler,\n",
        "                                                                            avg_train_loss_list, avg_val_loss_list,\n",
        "                                                                            TRAIN_BATCH_SIZE, VAL_BATCH_SIZE,\n",
        "                                                                            activate_scheduler=False, patience=PATIENCE, model_name=hsi_unet_pca_name,\n",
        "                                                                            data_source = source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hmD0DNubgnH_"
      },
      "outputs": [],
      "source": [
        "trained_model, loss, avg_train_loss_list, avg_val_loss_list = sf_model_training_multiloss(trained_model, train_loader_final, val_loader_final, NUM_EPOCHS_FINAL,\n",
        "                                                                            ce_loss_fn, dice_loss_fn, optimizer, scaler, scheduler,\n",
        "                                                                            avg_train_loss_list, avg_val_loss_list,\n",
        "                                                                            TRAIN_BATCH_SIZE, VAL_BATCH_SIZE,\n",
        "                                                                            activate_scheduler=False, patience=0, model_name=hsi_unet_pca_name,\n",
        "                                                                            data_source = source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_0eHIAAtgnH_"
      },
      "outputs": [],
      "source": [
        "#plot_range = range(NUM_EPOCHS+NUM_EPOCHS_FINAL)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(9,6))\n",
        "ax.plot(range(len(avg_train_loss_list)), avg_train_loss_list, marker='o', linestyle='-', label='Training Loss', color='blue')\n",
        "\n",
        "# Create a twin Axes sharing the xaxis\n",
        "ax2 = ax.twinx()\n",
        "ax2.plot(range(len(avg_val_loss_list)), avg_val_loss_list, marker='o', linestyle='-', label='Validation Loss', color='orange')\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('Training Loss', color='blue')\n",
        "ax2.set_ylabel('Validation Loss', color='orange')\n",
        "ax.set_title('Training vs Validation Loss')\n",
        "\n",
        "# Show legend for both axes\n",
        "ax.legend(loc='upper left')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation - HSI GELU Model - Final Model"
      ],
      "metadata": {
        "id": "Xz10eofwqnGD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YTh7jCxugnH_"
      },
      "outputs": [],
      "source": [
        "test_ds_union, test_ds_intersection, test_ds_numerator, test_ds_denominator, iou_image_pixelwise, dice_image_pixelwise = capture_model_metrics_pixelwise_and_confusion_matrix_sf(trained_model, test_dataset_final, data_source='hsi', visualize = VISUALIZE, mask_shape = (384, 320))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation - HSI GELU Model - Final Model"
      ],
      "metadata": {
        "id": "XZ-KE7Q9qtSx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GZyNxtmQgnH_"
      },
      "outputs": [],
      "source": [
        "calculate_model_metrics(test_ds_intersection, test_ds_union, test_ds_numerator, test_ds_denominator, defects_only=True, file_name=hsi_unet_pca_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkkI3_BhgnH_"
      },
      "source": [
        "# Evaluation - HSI GELU Model - Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "L3ARuNu-gnH_"
      },
      "outputs": [],
      "source": [
        "path = '/kaggle/working/'\n",
        "model_name = f'best_model_{_today}_{hsi_unet_pca_name}.pt'\n",
        "model_path = os.path.join(path, model_name)\n",
        "best_model_unet_hsi_pca = load_model(hsi_unet_model_gelu_pca(6), optimizer, scaler, model_path)\n",
        "\n",
        "test_ds_union, test_ds_intersection, test_ds_numerator, test_ds_denominator, iou_image_pixelwise, dice_image_pixelwise = capture_model_metrics_pixelwise_and_confusion_matrix_sf(best_model_unet_hsi_pca, test_dataset_final, data_source='hsi', visualize=VISUALIZE, mask_shape = (384, 320))\n",
        "\n",
        "calculate_model_metrics(test_ds_intersection, test_ds_union, test_ds_numerator, test_ds_denominator, defects_only=True, file_name=hsi_unet_pca_name)\n",
        "\n",
        "\n",
        "calculate_model_inference_time(best_model_unet_hsi_pca, val_loader_final, 'hsi')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ijm-7Ebv0ga_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post Processing pre Argmax"
      ],
      "metadata": {
        "id": "zhPSZVhR0hBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds_union = [0,0,0,0,0,0,0,0,0,0]\n",
        "test_ds_intersection = [0,0,0,0,0,0,0,0,0,0]\n",
        "test_ds_numerator = [0,0,0,0,0,0,0,0,0,0]\n",
        "test_ds_denominator = [0,0,0,0,0,0,0,0,0,0]\n",
        "kernel_size = [1,12,12,1,50,8,6,10,1,4]*2\n",
        "\n",
        "def probability_based_kernel_post_processing(model, smooth, dataset, kernel_size):\n",
        "\n",
        "    #create array to capture all ground truth and predictions to calculate final IoU and Dice Score at the end\n",
        "    ground_truth_all_images=np.zeros((mask_shape[0], mask_shape[1], len(test_dataset_final)))\n",
        "    prediction_all_images=np.zeros((mask_shape[0], mask_shape[1], len(test_dataset_final)))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for n, batch in enumerate(dataset):\n",
        "\n",
        "            #empty numpy mask to fit the one hot encoded classes\n",
        "            pp_one_hot_pred_masks = np.zeros((384,320, 10))\n",
        "\n",
        "            rgb_img, hsi_img, mask = batch\n",
        "\n",
        "            #predict imgs in dataset\n",
        "            rgb_img = rgb_img.to(DEVICE).unsqueeze(0)\n",
        "            mask = mask.to(DEVICE)\n",
        "            softmax = nn.Softmax(dim=1)\n",
        "\n",
        "            #predicted probabilities\n",
        "            preds = softmax(model(rgb_img.float())).to('cpu').squeeze(0).permute(1,2,0)\n",
        "            preds_np = preds.numpy()\n",
        "\n",
        "            #combined masks for comparison\n",
        "            preds_argmax = torch.argmax(preds, axis=-1).to('cpu').squeeze(0)\n",
        "\n",
        "            start = time.time()\n",
        "\n",
        "            # individual kernels for each defect class\n",
        "            for i in range(9, 2, -1):\n",
        "                kernel = np.ones((kernel_size[i],kernel_size[i]),np.uint8)\n",
        "                radius = int(kernel_size[i]/2)\n",
        "\n",
        "                #elliptical kernel size\n",
        "                kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2 * radius + 1, 2 * radius + 1))\n",
        "\n",
        "                #closing operation\n",
        "                pp_one_hot_pred_masks[:,:,i] = cv2.morphologyEx(preds_np[:,:,i], cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "            # individual kernels for each background, fillet front and back class\n",
        "            for i in range(2, -1, -1):\n",
        "                kernel = np.ones((kernel_size[i],kernel_size[i]),np.uint8)\n",
        "                radius = int(kernel_size[i]/2)\n",
        "                kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2 * radius + 1, 2 * radius + 1))\n",
        "\n",
        "                #dilation operation\n",
        "                pp_one_hot_pred_masks[:,:,i] = cv2.dilate(preds_np[:,:,i], kernel)\n",
        "\n",
        "            end = time.time()\n",
        "            print(f'Time: {end - start}')\n",
        "\n",
        "            # convert back to torch because evaluation functions only work with torch tensors\n",
        "            pp_one_hot_pred_masks = torch.from_numpy(pp_one_hot_pred_masks).to('cpu')\n",
        "\n",
        "            # combine post processed masks\n",
        "            single_mask = np.argmax(pp_one_hot_pred_masks, axis=-1)\n",
        "\n",
        "            # add current mask and prediction to stacked array for\n",
        "            prediction_all_images[:,:,n] = single_mask\n",
        "            ground_truth_all_images[:,:,n] = mask.to('cpu').numpy()\n",
        "\n",
        "            #calculate dice and iou score for calculating final IoU and Dice Score at the end\n",
        "            is_list, u_list=intersection_and_union_all_classes(mask, single_mask, SINGLE_PREDICTION=True)\n",
        "            n_list, d_list=dice_values_all_classes(mask, single_mask, SINGLE_PREDICTION=True)\n",
        "\n",
        "            #visualize predictions vs ground truth\n",
        "            visualize_prediction_vs_ground_truth_overlay_all_sources_postprocessing(rgb_img.squeeze(0), hsi_img.squeeze(0), mask, preds_argmax.squeeze(0), single_mask, 'rgb')\n",
        "\n",
        "            #print iou and dice score for each individual image\n",
        "            print('IOU')\n",
        "            for i in range(len(NUM_UNIQUE_VALUES_LONG)):\n",
        "                if is_ground_truth_empty(mask)[i] and is_prediction_empty(single_mask)[i]:\n",
        "                    print(f'{TXT_COLORS_LONG_COLOR_ONLY[i]} - {CLASSES_LONG[i]}: Empty')\n",
        "                else:\n",
        "                    print(f'{TXT_COLORS_LONG_COLOR_ONLY[i]} - {CLASSES_LONG[i]}: {is_list[i]/(u_list[i]+smooth):.4f}')\n",
        "                    #tracking class average of iou across all images\n",
        "                    test_ds_union[i] += u_list[i]\n",
        "                    test_ds_intersection[i] += is_list[i]\n",
        "\n",
        "            print(TXT_COLORS_LONG_COLOR_ONLY[0]+ 'x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x')\n",
        "            print('Dice')\n",
        "            for i in range(len(NUM_UNIQUE_VALUES_LONG)):\n",
        "                if is_ground_truth_empty(mask)[i] and is_prediction_empty(single_mask)[i]:\n",
        "                    print(f'{TXT_COLORS_LONG_COLOR_ONLY[i]} - {CLASSES_LONG[i]}: Empty')\n",
        "                else:\n",
        "                    print(f'{TXT_COLORS_LONG_COLOR_ONLY[i]} - {CLASSES_LONG[i]}: {n_list[i]/(d_list[i]+smooth):.4f}')\n",
        "                    #tracking class average of iou across all images\n",
        "                    test_ds_numerator[i] += n_list[i]\n",
        "                    test_ds_denominator[i] += d_list[i]\n",
        "\n",
        "                print(TXT_COLORS_LONG_COLOR_ONLY[0])\n",
        "\n",
        "    #evaluate overall model\n",
        "    calculate_model_metrics(test_ds_intersection, test_ds_union, test_ds_numerator, test_ds_denominator, defects_only=True)\n",
        "\n",
        "probability_based_kernel_post_processing(model, smooth, test_dataset_final, kernel_size)"
      ],
      "metadata": {
        "id": "LcSlvypkxbI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post Processing after argmax"
      ],
      "metadata": {
        "id": "GafBf4Tx5pFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds_union = [0,0,0,0,0,0,0,0,0,0]\n",
        "test_ds_intersection = [0,0,0,0,0,0,0,0,0,0]\n",
        "test_ds_numerator = [0,0,0,0,0,0,0,0,0,0]\n",
        "test_ds_denominator = [0,0,0,0,0,0,0,0,0,0]\n",
        "kernel_size = [1,20,20,1,50,8,6,16,1,4]\n",
        "smooth=1e-8\n",
        "\n",
        "def region_based_kernel_post_processing(model, smooth, dataset, kernel_size):\n",
        "\n",
        "    #create array to capture all ground truth and predictions to calculate final IoU and Dice Score at the end\n",
        "    ground_truth_all_images=np.zeros((mask_shape[0], mask_shape[1], len(dataset)))\n",
        "    prediction_all_images=np.zeros((mask_shape[0], mask_shape[1], len(dataset)))\n",
        "    pp_one_hot_pred_masks = np.zeros((384,320, 10),np.uint8)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for n, batch in enumerate(dataset):\n",
        "            rgb_img, hsi_img, mask = batch\n",
        "\n",
        "            rgb_img = rgb_img.to(DEVICE).unsqueeze(0)\n",
        "            mask = mask.to(DEVICE)\n",
        "            softmax = nn.Softmax(dim=1)\n",
        "            preds = torch.argmax(softmax(model(rgb_img.float())),axis=1).to('cpu').squeeze(0)\n",
        "\n",
        "            #one hot encoding of mask after argmax\n",
        "            one_hot_pred_masks=F.one_hot(preds.to(torch.int64), num_classes=10).to(DEVICE)\n",
        "\n",
        "            # individual kernels for each defect class\n",
        "            for i in range(9, 2, -1):\n",
        "                kernel = np.ones((kernel_size[i],kernel_size[i]),np.uint8)\n",
        "                pp_one_hot_pred_masks[:,:,i] = cv2.morphologyEx(one_hot_pred_masks[:,:,i].to('cpu').numpy().astype(np.uint8), cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "            # individual kernels for each background, fillet front and back class\n",
        "            for i in range(2, -1, -1):\n",
        "                kernel = np.ones((kernel_size[i],kernel_size[i]),np.uint8)\n",
        "                pp_one_hot_pred_masks[:,:,i] = cv2.morphologyEx(one_hot_pred_masks[:,:,i].to('cpu').numpy().astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "            #give defect class and fillet front and back more weight than background class\n",
        "            pp_one_hot_pred_masks[:,:,1:3] = pp_one_hot_pred_masks[:,:,1:3]*2\n",
        "            pp_one_hot_pred_masks[:,:,3:10] = pp_one_hot_pred_masks[:,:,3:10]*3\n",
        "\n",
        "            #combine mask\n",
        "            single_mask_array = np.argmax(pp_one_hot_pred_masks, axis=-1)\n",
        "            single_mask = torch.from_numpy(single_mask_array)\n",
        "\n",
        "            # add current mask and prediction to stacked array for confusion matrix\n",
        "            prediction_all_images[:,:,n] = single_mask_array\n",
        "            ground_truth_all_images[:,:,n] = mask.to('cpu').numpy()\n",
        "\n",
        "            #calculate dice and iou score for calculating final IoU and Dice Score at the end\n",
        "            is_list, u_list=intersection_and_union_all_classes(mask, single_mask, SINGLE_PREDICTION=True)\n",
        "            n_list, d_list=dice_values_all_classes(mask, single_mask, SINGLE_PREDICTION=True)\n",
        "\n",
        "            #visualize predictions vs ground truth\n",
        "            visualize_prediction_vs_ground_truth_overlay_all_sources_postprocessing(rgb_img.squeeze(0), hsi_img.squeeze(0), mask, preds.squeeze(0), single_mask, 'rgb')\n",
        "\n",
        "            #print iou and dice score for each individual image\n",
        "            print('IOU')\n",
        "            for i in range(len(NUM_UNIQUE_VALUES_LONG)):\n",
        "                if is_ground_truth_empty(mask)[i] and is_prediction_empty(single_mask)[i]:\n",
        "                    print(f'{TXT_COLORS_LONG_COLOR_ONLY[i]} - {CLASSES_LONG[i]}: Empty')\n",
        "                else:\n",
        "                    print(f'{TXT_COLORS_LONG_COLOR_ONLY[i]} - {CLASSES_LONG[i]}: {is_list[i]/(u_list[i]+smooth):.4f}')\n",
        "                    #tracking class average of iou across all images\n",
        "                    test_ds_union[i] += u_list[i]\n",
        "                    test_ds_intersection[i] += is_list[i]\n",
        "\n",
        "            print(TXT_COLORS_LONG_COLOR_ONLY[0]+ 'x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x')\n",
        "            print('Dice')\n",
        "            for i in range(len(NUM_UNIQUE_VALUES_LONG)):\n",
        "                if is_ground_truth_empty(mask)[i] and is_prediction_empty(single_mask)[i]:\n",
        "                    print(f'{TXT_COLORS_LONG_COLOR_ONLY[i]} - {CLASSES_LONG[i]}: Empty')\n",
        "                else:\n",
        "                    print(f'{TXT_COLORS_LONG_COLOR_ONLY[i]} - {CLASSES_LONG[i]}: {n_list[i]/(d_list[i]+smooth):.4f}')\n",
        "                    #tracking class average of iou across all images\n",
        "                    test_ds_numerator[i] += n_list[i]\n",
        "                    test_ds_denominator[i] += d_list[i]\n",
        "\n",
        "            print(TXT_COLORS_LONG_COLOR_ONLY[0])\n",
        "\n",
        "    calculate_model_metrics(test_ds_intersection, test_ds_union, test_ds_numerator, test_ds_denominator, defects_only=True)\n",
        "\n",
        "region_based_kernel_post_processing(model, smooth, test_dataset_final, kernel_size)"
      ],
      "metadata": {
        "id": "xJ5exr6Sy41E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CRF-based Post Processing"
      ],
      "metadata": {
        "id": "nKGOrUAWgyBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pydensecrf\n",
        "import pydensecrf.densecrf as dcrf\n",
        "from pydensecrf.utils import unary_from_labels, create_pairwise_bilateral, create_pairwise_gaussian\n",
        "\n",
        "test_ds_union = [0,0,0,0,0,0,0,0,0,0]\n",
        "test_ds_intersection = [0,0,0,0,0,0,0,0,0,0]\n",
        "test_ds_numerator = [0,0,0,0,0,0,0,0,0,0]\n",
        "test_ds_denominator = [0,0,0,0,0,0,0,0,0,0]\n",
        "mask_shape = (384,320)\n",
        "smooth=1e-8\n",
        "model = gelu_model\n",
        "\n",
        "theta_alpha = 20\n",
        "theta_beta = 15\n",
        "theta_gamma = 6\n",
        "\n",
        "def crf_based_post_processing(model, smooth, dataset, mask_shape, theta_a, theta_b, theta_g):\n",
        "\n",
        "    #create array to capture all ground truth and predictions to calculate final IoU and Dice Score at the end\n",
        "    ground_truth_all_images=np.zeros((mask_shape[0], mask_shape[1], len(test_dataset_final)))\n",
        "    prediction_all_images=np.zeros((mask_shape[0], mask_shape[1], len(test_dataset_final)))\n",
        "    pp_one_hot_pred_masks = np.zeros((384,320, 10),np.uint8)\n",
        "\n",
        "    #model inference\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for n, batch in enumerate(test_dataset_final):\n",
        "            rgb_img, hsi_img, mask = batch\n",
        "\n",
        "            rgb_img = rgb_img.to(DEVICE).unsqueeze(0)\n",
        "            mask = mask.to(DEVICE)\n",
        "            softmax = nn.Softmax(dim=1)\n",
        "            preds = torch.argmax(softmax(model(rgb_img.float())),axis=1).to('cpu').squeeze(0)\n",
        "\n",
        "            #convert original img and annotated img into numpy arrays\n",
        "            original_image=rgb_img.to('cpu').squeeze().permute(1,2,0).numpy()\n",
        "            annotated_image=preds.to('cpu').numpy().astype(np.uint32)\n",
        "\n",
        "            #from here: code snippets from git@github.com:lucasb-eyer/pydensecrf.git\n",
        "            #and from here: code snippets from git@github.com:dhawan98/Post-Processing-of-Image-Segmentation-using-CRF.git\n",
        "            #number of classes in dataset\n",
        "            n_labels_a = 10\n",
        "\n",
        "            #flatten segmentation mask\n",
        "            labels_a = annotated_image.flatten()\n",
        "\n",
        "            #Setting up the CRF model\n",
        "            d = dcrf.DenseCRF2D(original_image.shape[1], original_image.shape[0], n_labels_a)\n",
        "\n",
        "            # get unary potentials (neg log probability)\n",
        "            U = unary_from_labels(labels_a, n_labels_a, gt_prob=0.90, zero_unsure=False)\n",
        "\n",
        "            #calculate Gibbs energy\n",
        "            d.setUnaryEnergy(U)\n",
        "\n",
        "            # This adds the color-independent term, features are the locations only.\n",
        "            # smoothing kernel\n",
        "            d.addPairwiseGaussian(sxy=(theta_gamma, theta_gamma), compat=3, kernel=dcrf.DIAG_KERNEL,\n",
        "                              normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
        "\n",
        "            # This adds the color-dependent term, i.e. features are (x,y,r,g,b).\n",
        "            # appearance kernel\n",
        "            d.addPairwiseBilateral(sxy=(theta_alpha, theta_alpha), srgb=(theta_beta, theta_beta, theta_beta), rgbim=original_image.astype(np.uint8),\n",
        "                               compat=10,\n",
        "                               kernel=dcrf.DIAG_KERNEL,\n",
        "                               normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
        "\n",
        "            #Run CRF model inference for x steps\n",
        "            Q = d.inference(1)\n",
        "\n",
        "            # Find out the most probable class for each pixel.\n",
        "            MAP = np.argmax(Q, axis=0)\n",
        "\n",
        "            # Convert the MAP (labels) back to the corresponding colors and save the image.\n",
        "            post_processed_mask=MAP.reshape(annotated_image.shape)\n",
        "\n",
        "            ####\n",
        "            #code snippets from github repos end here\n",
        "\n",
        "            #convert mask back to torch tensor for evaluating purposes\n",
        "            single_mask = torch.from_numpy(post_processed_mask)\n",
        "\n",
        "            #calculate dice and iou score for calculating final IoU and Dice Score at the end\n",
        "            is_list, u_list=intersection_and_union_all_classes(mask, single_mask, SINGLE_PREDICTION=True)\n",
        "            n_list, d_list=dice_values_all_classes(mask, single_mask, SINGLE_PREDICTION=True)\n",
        "\n",
        "            #visualize predictions vs ground truth\n",
        "            visualize_prediction_vs_ground_truth_overlay_all_sources_postprocessing(rgb_img.squeeze(0), hsi_img.squeeze(0), mask, preds.squeeze(0), single_mask, 'rgb')\n",
        "\n",
        "            #print iou and dice score for each individual image\n",
        "            print('IOU')\n",
        "            for i in range(len(NUM_UNIQUE_VALUES_LONG)):\n",
        "                if is_ground_truth_empty(mask)[i] and is_prediction_empty(single_mask)[i]:\n",
        "                    print(f'{TXT_COLORS_LONG_COLOR_ONLY[i]} - {CLASSES_LONG[i]}: Empty')\n",
        "                else:\n",
        "                    print(f'{TXT_COLORS_LONG_COLOR_ONLY[i]} - {CLASSES_LONG[i]}: {is_list[i]/(u_list[i]+smooth):.4f}')\n",
        "                    #tracking class average of iou across all images\n",
        "                    test_ds_union[i] += u_list[i]\n",
        "                    test_ds_intersection[i] += is_list[i]\n",
        "\n",
        "            print(TXT_COLORS_LONG_COLOR_ONLY[0]+ 'x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x')\n",
        "            print('Dice')\n",
        "            for i in range(len(NUM_UNIQUE_VALUES_LONG)):\n",
        "                if is_ground_truth_empty(mask)[i] and is_prediction_empty(single_mask)[i]:\n",
        "                    print(f'{TXT_COLORS_LONG_COLOR_ONLY[i]} - {CLASSES_LONG[i]}: Empty')\n",
        "                else:\n",
        "                    print(f'{TXT_COLORS_LONG_COLOR_ONLY[i]} - {CLASSES_LONG[i]}: {n_list[i]/(d_list[i]+smooth):.4f}')\n",
        "                    #tracking class average of iou across all images\n",
        "                    test_ds_numerator[i] += n_list[i]\n",
        "                    test_ds_denominator[i] += d_list[i]\n",
        "\n",
        "            print(TXT_COLORS_LONG_COLOR_ONLY[0])\n",
        "\n",
        "        calculate_model_metrics(test_ds_intersection, test_ds_union, test_ds_numerator, test_ds_denominator, defects_only=True)\n",
        "\n",
        "crf_based_post_processing(model, smooth, test_dataset_final, mask_shape, theta_alpha, theta_beta, theta_gamma)"
      ],
      "metadata": {
        "id": "rkd1UID7xbGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kE3UwiY4xbD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ofRDRxFSxbBP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5155186,
          "sourceId": 8616507,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5184721,
          "sourceId": 8654841,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30699,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}